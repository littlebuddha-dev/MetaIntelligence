# /cli/main.py
# ã‚¿ã‚¤ãƒˆãƒ«: CLI main entrypoint with Corrected Argument Passing and Syntax Fix
# å½¹å‰²: CLIã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨å¼•æ•°è§£æã€‚æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã€å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’æ”¹å–„ã€‚

import argparse
import asyncio
import json
import logging
import os
import sys

from dotenv import load_dotenv
load_dotenv()

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from llm_api.config import settings
from cli.handler import CogniQuantumCLIV2Fixed
from llm_api.providers import list_providers, list_enhanced_providers
from llm_api.utils.helper_functions import format_json_output, read_from_pipe_or_file

logger = logging.getLogger(__name__)

async def main():
    parser = argparse.ArgumentParser(
        description="CogniQuantum V2çµ±åˆLLM CLIï¼ˆè¨­å®šç®¡ç†æ”¹å–„ç‰ˆï¼‰",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument("provider", nargs='?', help="ä½¿ç”¨ã™ã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼")
    parser.add_argument("prompt", nargs='?', default=None, help="LLMã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    
    mode_choices = [
        'simple', 'chat', 'reasoning', 'creative-fusion', 'self-correct',
        'efficient', 'balanced', 'decomposed', 'adaptive', 'paper_optimized', 'parallel',
        'quantum_inspired', 'edge', 'speculative_thought', 'self_discover'
    ]
    parser.add_argument("--mode", default=settings.V2_DEFAULT_MODE, choices=mode_choices, help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰")
    
    parser.add_argument("--model", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¯ã«è¨­å®šï¼‰")
    parser.add_argument("-f", "--file", help="ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿")
    parser.add_argument("--system-prompt", help="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    parser.add_argument("--temperature", type=float, help="ç”Ÿæˆã®å¤šæ§˜æ€§")
    parser.add_argument("--max-tokens", type=int, help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°")
    parser.add_argument("--json", action="store_true", help="JSONå‡ºåŠ›")
    
    parser.add_argument("--list-providers", action="store_true", help="ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§è¡¨ç¤º")
    parser.add_argument("--system-status", action="store_true", help="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º")
    parser.add_argument("--health-check", action="store_true", help="å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
    parser.add_argument("--troubleshooting", action="store_true", help="ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰")
    
    v2_group = parser.add_argument_group('V2 Options')
    v2_group.add_argument("--force-v2", action="store_true", help="V2æ©Ÿèƒ½å¼·åˆ¶ä½¿ç”¨")
    v2_group.add_argument("--no-fallback", action="store_true", help="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç„¡åŠ¹")
    v2_group.add_argument("--no-real-time-adjustment", dest="real_time_adjustment", action="store_false", help="ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¤‡é›‘æ€§èª¿æ•´ã‚’ç„¡åŠ¹åŒ–")

    rag_group = parser.add_argument_group('RAG Options')
    rag_group.add_argument("--rag", dest="use_rag", action="store_true", help="RAGæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–")
    rag_group.add_argument("--knowledge-base", dest="knowledge_base_path", help="RAGãŒä½¿ç”¨ã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹")
    rag_group.add_argument("--wikipedia", dest="use_wikipedia", action="store_true", help="RAGã§Wikipediaã‚’ä½¿ç”¨")

    args = parser.parse_args()

    cli = CogniQuantumCLIV2Fixed()

    if args.list_providers:
        print("æ¨™æº–ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:", ", ".join(list_providers()))
        enhanced_info = list_enhanced_providers()
        print("æ‹¡å¼µãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ V2:", ", ".join(enhanced_info['v2']))
        return

    if args.system_status:
        cli.print_system_status()
        return

    if args.troubleshooting:
        cli.print_troubleshooting_guide()
        return

    if not args.provider:
        parser.print_help()
        sys.exit(1)

    # --- å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã®å‡¦ç†ã‚’å…ˆã«å®Ÿè¡Œ ---
    if args.health_check:
        try:
            health_report = await cli.check_system_health(args.provider)
            print(format_json_output(health_report) if args.json else json.dumps(health_report, indent=2, ensure_ascii=False))
        except Exception as e:
            print(f"å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            return # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ãŒæŒ‡å®šã•ã‚ŒãŸã‚‰ã€ã“ã“ã§å‡¦ç†ã‚’çµ‚äº†

    # --- ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ ---
    is_available = True
    if args.provider == 'ollama':
        ollama_health = await cli._check_ollama_models()
        if not ollama_health.get('server_available'):
            is_available = False
    else:
        key_map = {
            'openai': settings.OPENAI_API_KEY, 
            'claude': settings.CLAUDE_API_KEY, 
            'gemini': settings.GEMINI_API_KEY, 
            'huggingface': settings.HF_TOKEN
        }
        if args.provider in key_map and not key_map.get(args.provider):
            print(f"è­¦å‘Š: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{args.provider}' ã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            is_available = False
    
    if not is_available:
        print("ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        sys.exit(1)

    # --- ãƒ¡ã‚¤ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç† ---
    prompt = await read_from_pipe_or_file(args.prompt, args.file)
    if not prompt:
        parser.error("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚--health-check ãªã©ã®ç®¡ç†ã‚³ãƒãƒ³ãƒ‰ã§ã¯ãªã„å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¿…é ˆã§ã™ã€‚")

    # kwargsã‚’æ§‹ç¯‰
    kwargs = {k: v for k, v in vars(args).items() if v is not None}
    kwargs.pop('provider', None)
    kwargs.pop('prompt', None)

    try:
        response = await cli.process_request_with_fallback(
            args.provider, prompt, **kwargs
        )
        
        if args.json:
            print(format_json_output(response))
        else:
            text_output = response.get("text", "")
            print(text_output, end='')
            
            if response.get('image_url'):
                print(f"\n\né–¢é€£ç”»åƒ: {response['image_url']}")

            # ã‚¨ãƒ©ãƒ¼ã‚„V2æƒ…å ±ãªã©ã€è£œè¶³æƒ…å ±ãŒã‚ã‚‹å ´åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
            if response.get('error') or response.get('fallback_used') or response.get('version') == 'v2':
                print() 

            if response.get('error'):
                print(f"\nâš ï¸  ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                if response.get('all_errors'):
                    print("è©³ç´°ã‚¨ãƒ©ãƒ¼:")
                    for i, error in enumerate(response['all_errors'], 1):
                        print(f"  {i}. {error}")
                
                if response.get('suggestions'):
                    print("\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
                    for suggestion in response['suggestions']:
                        print(f"  â€¢ {suggestion}")
            
            elif response.get('fallback_used'):
                print(f"\nâœ“ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ: {response.get('fallback_type')}")
                if response.get('original_errors'):
                    print("å…ƒã®ã‚¨ãƒ©ãƒ¼:")
                    for error in response['original_errors']:
                        print(f"  â€¢ {error}")
            
            elif response.get('version') == 'v2':
                v2_info = response.get('v2_improvements', {})
                print(f"\nğŸ“Š V2å‡¦ç†æƒ…å ±:")
                print(f"  è¤‡é›‘æ€§ä½“åˆ¶: {v2_info.get('regime', 'N/A')}")
                print(f"  æ¨è«–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {v2_info.get('reasoning_approach', 'N/A')}")
                
                # V2 Improvementsã®è©³ç´°è¡¨ç¤º
                if v2_info.get('overthinking_prevention'): print("  âœ“ Overthinkingé˜²æ­¢æœ‰åŠ¹")
                if v2_info.get('collapse_prevention'): print("  âœ“ å´©å£Šé˜²æ­¢æ©Ÿæ§‹æœ‰åŠ¹")
                if v2_info.get('real_time_adjustment_active'): print("  âœ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¤‡é›‘æ€§èª¿æ•´æœ‰åŠ¹")
                if v2_info.get('rag_enabled'):
                    rag_source = "Wikipedia" if v2_info.get('rag_source') == 'wikipedia' else 'Knowledge Base'
                    print(f"  âœ“ RAGã«ã‚ˆã‚‹çŸ¥è­˜æ‹¡å¼µæœ‰åŠ¹ (ã‚½ãƒ¼ã‚¹: {rag_source})")
                if v2_info.get('strategy_used'): print(f"  âœ“ è‡ªå·±ç™ºè¦‹æˆ¦ç•¥: {v2_info.get('strategy_used')}")
                if v2_info.get('speculative_execution_enabled'): print(f"  âœ“ æŠ•æ©Ÿçš„å®Ÿè¡Œæœ‰åŠ¹ (ãƒ‰ãƒ©ãƒ•ãƒˆãƒ¢ãƒ‡ãƒ«: {v2_info.get('draft_model')})")


    except KeyboardInterrupt:
        print("\nä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        logger.critical(f"äºˆæœŸã—ãªã„è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        print(f"\näºˆæœŸã—ãªã„è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    from llm_api import setup_logging
    setup_logging()
    asyncio.run(main())