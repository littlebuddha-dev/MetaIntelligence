# /examples/autonomous_learning_demo.py
"""
è‡ªå¾‹Webå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨ä¾‹ã¨ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
"""

import asyncio
import logging
from llm_api.providers import get_provider
from llm_api.autonomous_learning.web_crawler import (
    AutonomousWebCrawler, 
    ContinuousLearningManager,
    InterestProfiler
)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def demo_autonomous_learning():
    """è‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢"""
    
    # 1. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–
    provider = get_provider("ollama", enhanced=True)
    
    # 2. Webæ¤œç´¢ãƒ»å–å¾—æ©Ÿèƒ½ã®æº–å‚™ï¼ˆå®Ÿéš›ã«ã¯web_searchã¨web_fetché–¢æ•°ãŒå¿…è¦ï¼‰
    async def mock_web_search(query):
        return {
            'results': [
                {'link': 'https://example.com/ai-research-1', 'title': 'Latest AI Research'},
                {'link': 'https://example.com/ml-breakthrough', 'title': 'ML Breakthrough'},
                {'link': 'https://example.com/cognitive-science', 'title': 'Cognitive Science'}
            ]
        }
    
    async def mock_web_fetch(url):
        return {
            'content': f"This is sample content from {url}. It contains information about artificial intelligence, machine learning, and cognitive processes.",
            'title': f"Sample Title for {url}"
        }
    
    # 3. è‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    crawler = AutonomousWebCrawler(provider, mock_web_search, mock_web_fetch)
    
    # 4. å˜ç™ºã®è‡ªå¾‹å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³
    logger.info("=== å˜ç™ºè‡ªå¾‹å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¢ ===")
    
    initial_topics = [
        "artificial intelligence", 
        "consciousness studies", 
        "neural networks",
        "AI safety"
    ]
    
    session_result = await crawler.start_autonomous_learning(
        initial_topics=initial_topics,
        session_duration=300  # 5åˆ†é–“ã®ãƒ‡ãƒ¢
    )
    
    print(f"å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†:")
    print(f"- æ¢ç´¢ãƒšãƒ¼ã‚¸æ•°: {session_result['pages_crawled']}")
    print(f"- ç™ºè¦‹ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(session_result['session_summary']['content_discovered'])}")
    print(f"- ç²å¾—ã—ãŸçŸ¥è­˜: {len(session_result['session_summary']['knowledge_gained'])}")
    print(f"- å­¦ç¿’åŠ¹ç‡: {session_result['learning_efficiency']:.2f}")
    
    # 5. ç¶™ç¶šå­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ‡ãƒ¢
    logger.info("\n=== ç¶™ç¶šå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ ===")
    
    learning_manager = ContinuousLearningManager(provider, mock_web_search, mock_web_fetch)
    
    # ç¶™ç¶šå­¦ç¿’ã®è¨­å®š
    setup_result = await learning_manager.setup_continuous_learning(
        learning_intervals={
            "daily_exploration": 1800,    # 30åˆ†
            "weekly_deep_dive": 3600,     # 1æ™‚é–“
            "monthly_review": 7200        # 2æ™‚é–“
        },
        learning_goals=[
            "æœ€æ–°ã®AIç ”ç©¶å‹•å‘ã®æŠŠæ¡",
            "èªçŸ¥ç§‘å­¦ã®æ–°ç™ºè¦‹ã®å­¦ç¿’", 
            "AIå®‰å…¨æ€§ã«é–¢ã™ã‚‹è­°è«–ã®è¿½è·¡",
            "å®Ÿç”¨çš„ãªAIæŠ€è¡“ã®ç™ºè¦‹"
        ]
    )
    
    print(f"ç¶™ç¶šå­¦ç¿’è¨­å®šå®Œäº†:")
    print(f"- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {setup_result['learning_schedule']}")
    print(f"- å­¦ç¿’ç›®æ¨™: {len(setup_result['learning_goals'])}å€‹")
    
    # æ—¥æ¬¡æ¢ç´¢ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
    daily_session = await learning_manager.execute_scheduled_learning("daily_exploration")
    
    print(f"\næ—¥æ¬¡æ¢ç´¢ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†:")
    print(f"- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—: {daily_session['session_type']}")
    print(f"- å­¦ç¿’åŠ¹ç‡: {daily_session['learning_result']['learning_efficiency']:.2f}")

async def demo_interest_profiling():
    """èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãƒ‡ãƒ¢"""
    logger.info("=== èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãƒ‡ãƒ¢ ===")
    
    provider = get_provider("ollama", enhanced=True)
    profiler = InterestProfiler(provider)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è©•ä¾¡
    sample_contents = [
        {
            'text': "æœ€æ–°ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹å‰µç™ºçš„èƒ½åŠ›ã«ã¤ã„ã¦ç ”ç©¶ã—ãŸçµæœã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒä¸€å®šã®é–¾å€¤ã‚’è¶…ãˆã‚‹ã¨äºˆæœŸã—ãªã„èƒ½åŠ›ãŒç¾ã‚Œã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚",
            'metadata': {'title': 'LLMã®å‰µç™ºçš„èƒ½åŠ›ã«é–¢ã™ã‚‹ç ”ç©¶'}
        },
        {
            'text': "ä»Šæ—¥ã®ãƒ©ãƒ³ãƒãƒ¡ãƒ‹ãƒ¥ãƒ¼ã¯ã‚«ãƒ¬ãƒ¼ãƒ©ã‚¤ã‚¹ã§ã—ãŸã€‚ã¨ã¦ã‚‚ç¾å‘³ã—ã‹ã£ãŸã§ã™ã€‚",
            'metadata': {'title': 'ãƒ©ãƒ³ãƒã®è¨˜éŒ²'}
        },
        {
            'text': "æ„è­˜ã®é›£ã—ã„å•é¡Œã«ã¤ã„ã¦ã€çµ±åˆæƒ…å ±ç†è«–ã®è¦³ç‚¹ã‹ã‚‰æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã™ã‚‹ã€‚",
            'metadata': {'title': 'æ„è­˜ç ”ç©¶ã®æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ'}
        }
    ]
    
    for i, content_data in enumerate(sample_contents, 1):
        interest_score, topics = await profiler.evaluate_content_interest(
            content_data['text'], 
            content_data['metadata']
        )
        
        print(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ {i}:")
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {content_data['metadata']['title']}")
        print(f"  èˆˆå‘³åº¦: {interest_score:.2f}")
        print(f"  é–¢é€£ãƒˆãƒ”ãƒƒã‚¯: {topics}")
        print()

async def demo_advanced_learning_features():
    """é«˜åº¦ãªå­¦ç¿’æ©Ÿèƒ½ã®ãƒ‡ãƒ¢"""
    logger.info("=== é«˜åº¦ãªå­¦ç¿’æ©Ÿèƒ½ã®ãƒ‡ãƒ¢ ===")
    
    provider = get_provider("ollama", enhanced=True)
    
    # Webæ¤œç´¢ã¨ãƒ•ã‚§ãƒƒãƒæ©Ÿèƒ½ï¼ˆå®Ÿéš›ã®å®Ÿè£…ä¾‹ï¼‰
    from llm_api.providers import get_provider
    
    async def real_web_search(query):
        """å®Ÿéš›ã®Webæ¤œç´¢æ©Ÿèƒ½ã®ä¾‹"""
        try:
            # å®Ÿéš›ã«ã¯web_searchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
            # ã“ã“ã§ã¯ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            return {
                'results': [
                    {
                        'link': f'https://arxiv.org/abs/2301.{i:05d}',
                        'title': f'Research Paper on {query} - {i}',
                        'snippet': f'Abstract about {query} research findings...'
                    }
                    for i in range(1, 6)
                ]
            }
        except Exception as e:
            logger.error(f"Web search error: {e}")
            return {'results': []}
    
    async def real_web_fetch(url):
        """å®Ÿéš›ã®Webãƒšãƒ¼ã‚¸å–å¾—æ©Ÿèƒ½ã®ä¾‹"""
        try:
            # å®Ÿéš›ã«ã¯web_fetchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
            # ã“ã“ã§ã¯ãƒ¢ãƒƒã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™
            return {
                'content': f"""
                This is a detailed research paper from {url}.
                
                Abstract: This paper presents novel findings in artificial intelligence
                and machine learning. We demonstrate new approaches to neural network
                architectures that show improved performance on various tasks.
                
                Introduction: Recent advances in deep learning have shown remarkable
                progress in various domains. However, several challenges remain...
                
                Methodology: We propose a new neural architecture that combines
                attention mechanisms with novel regularization techniques...
                
                Results: Our experiments show 15% improvement in accuracy compared
                to baseline methods. The proposed approach demonstrates strong
                generalization capabilities...
                
                Conclusion: This work opens new directions for future research
                in artificial intelligence and cognitive modeling.
                """,
                'title': f'Advanced AI Research - {url.split("/")[-1]}'
            }
        except Exception as e:
            logger.error(f"Web fetch error: {e}")
            return {'content': '', 'title': ''}
    
    # é«˜åº¦ãªè‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    advanced_crawler = AutonomousWebCrawler(provider, real_web_search, real_web_fetch)
    
    # å­¦ç¿’ç›®æ¨™ã®è¨­å®š
    learning_goals = [
        "æœ€æ–°ã®Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£",
        "ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã®é€²å±•",
        "AIå®‰å…¨æ€§ç ”ç©¶ã®å‹•å‘",
        "ç¥çµŒç§‘å­¦ã¨AIã®èåˆ",
        "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨AI"
    ]
    
    print("å­¦ç¿’ç›®æ¨™:")
    for i, goal in enumerate(learning_goals, 1):
        print(f"  {i}. {goal}")
    
    # é«˜åº¦ãªå­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
    advanced_session = await advanced_crawler.start_autonomous_learning(
        initial_topics=learning_goals,
        session_duration=600  # 10åˆ†é–“
    )
    
    print(f"\né«˜åº¦å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³çµæœ:")
    print(f"- ç·æ¢ç´¢æ™‚é–“: {advanced_session['duration']:.1f}ç§’")
    print(f"- ç™ºè¦‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ•°: {len(advanced_session['session_summary']['content_discovered'])}")
    print(f"- æ–°è¦çŸ¥è­˜é …ç›®: {len(advanced_session['session_summary']['knowledge_gained'])}")
    print(f"- å¹³å‡èˆˆå‘³åº¦: {_calculate_average_interest(advanced_session['session_summary']):.2f}")
    
    # å­¦ç¿’ã—ãŸçŸ¥è­˜ã®è¡¨ç¤º
    if advanced_session['session_summary']['knowledge_gained']:
        print(f"\nå­¦ç¿’ã—ãŸä¸»è¦çŸ¥è­˜:")
        for knowledge in advanced_session['session_summary']['knowledge_gained'][:5]:
            print(f"  - {knowledge}")
    
    # ç™ºè¦‹ã—ãŸèˆˆå‘³æ·±ã„ãƒˆãƒ”ãƒƒã‚¯
    if advanced_session['session_summary']['new_interests']:
        print(f"\næ–°ãŸã«ç™ºè¦‹ã—ãŸèˆˆå‘³æ·±ã„ãƒˆãƒ”ãƒƒã‚¯:")
        for topic in advanced_session['session_summary']['new_interests'][:5]:
            print(f"  - {topic}")

def _calculate_average_interest(session_summary):
    """å¹³å‡èˆˆå‘³åº¦ã®è¨ˆç®—"""
    content_discovered = session_summary.get('content_discovered', [])
    if not content_discovered:
        return 0.0
    
    total_interest = sum(content.get('interest_score', 0.0) for content in content_discovered)
    return total_interest / len(content_discovered)

async def demo_learning_integration():
    """å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã¨CogniQuantumã®çµ±åˆãƒ‡ãƒ¢"""
    logger.info("=== CogniQuantumçµ±åˆå­¦ç¿’ã®ãƒ‡ãƒ¢ ===")
    
    provider = get_provider("ollama", enhanced=True)
    
    # CogniQuantumã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆä¾‹
    from llm_api.cogniquantum.system import CogniQuantumSystemV2
    from llm_api.value_evolution.evolution_engine import ValueEvolutionEngine
    
    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    cq_system = CogniQuantumSystemV2(provider, {})
    value_system = ValueEvolutionEngine(provider)
    
    # ä¾¡å€¤ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    await value_system.initialize_core_values()
    
    # å­¦ç¿’ä½“é¨“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    learning_experience = {
        "context": {
            "type": "autonomous_web_learning",
            "session_duration": 600,
            "topics_explored": ["AI consciousness", "neural networks", "cognitive science"],
            "content_quality": "high"
        },
        "actions": [
            "æ¢ç´¢çš„Webæ¤œç´¢",
            "èˆˆå‘³åº¦è©•ä¾¡",
            "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ",
            "çŸ¥è­˜æŠ½å‡º",
            "æ¦‚å¿µçµ±åˆ"
        ],
        "outcomes": {
            "knowledge_acquired": 15,
            "concepts_learned": 8,
            "insights_generated": 3,
            "curiosity_satisfied": True
        },
        "satisfaction": 0.85  # é«˜ã„å­¦ç¿’æº€è¶³åº¦
    }
    
    # ä¾¡å€¤ã‚·ã‚¹ãƒ†ãƒ ã«å­¦ç¿’çµŒé¨“ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
    value_learning = await value_system.learn_from_experience(learning_experience)
    
    print("ä¾¡å€¤ã‚·ã‚¹ãƒ†ãƒ å­¦ç¿’çµæœ:")
    print(f"- ä¾¡å€¤èª¿æ•´æ•°: {len(value_learning['value_adjustments'])}")
    print(f"- å­¦ç¿’ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ: {value_learning['overall_impact']:.3f}")
    print(f"- ã‚·ã‚¹ãƒ†ãƒ ä¸€è²«æ€§: {value_learning['value_system_coherence']:.3f}")
    
    # å­¦ç¿’ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæ•™è¨“
    if value_learning['lessons_learned']:
        print(f"\nå­¦ç¿’ã—ãŸæ•™è¨“:")
        for lesson in value_learning['lessons_learned']:
            print(f"  - {lesson}")
    
    # CogniQuantumã‚·ã‚¹ãƒ†ãƒ ã§ã®çµ±åˆçš„å•é¡Œè§£æ±º
    integration_prompt = """
    è‡ªå¾‹çš„Webå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ä»¥ä¸‹ã®çŸ¥è­˜ã‚’ç²å¾—ã—ã¾ã—ãŸï¼š
    
    1. æœ€æ–°ã®AIç ”ç©¶ã§ã¯æ„è­˜ã®è¨ˆç®—ç†è«–ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹
    2. ç¥çµŒç§‘å­¦ã¨AIã®èåˆã«ã‚ˆã‚Šæ–°ã—ã„èªçŸ¥ãƒ¢ãƒ‡ãƒ«ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹
    3. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIãŒäººé–“é¡ä¼¼ã®å­¦ç¿’ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹
    
    ã“ã®æ–°ã—ã„çŸ¥è­˜ã‚’çµ±åˆã—ã¦ã€æ¬¡ã®æ¢ç´¢ã™ã¹ãç ”ç©¶é ˜åŸŸã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
    """
    
    integration_result = await cq_system.solve_problem(
        integration_prompt,
        mode="adaptive",
        use_rag=False
    )
    
    print(f"\nCogniQuantumçµ±åˆåˆ†æ:")
    print(f"æ¨å¥¨æ¢ç´¢é ˜åŸŸ: {integration_result.get('final_solution', '')[:200]}...")

# CLIãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®ä½¿ç”¨ä¾‹
async def cli_autonomous_learning():
    """CLIãƒ„ãƒ¼ãƒ«ã¨ã—ã¦ã®è‡ªå¾‹å­¦ç¿’"""
    import argparse
    
    parser = argparse.ArgumentParser(description="CogniQuantumè‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument("--mode", choices=["single", "continuous"], default="single",
                       help="å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument("--duration", type=int, default=1800,
                       help="å­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ï¼ˆç§’ï¼‰")
    parser.add_argument("--topics", nargs="+", default=["artificial intelligence"],
                       help="åˆæœŸæ¢ç´¢ãƒˆãƒ”ãƒƒã‚¯")
    parser.add_argument("--min-interest", type=float, default=0.6,
                       help="æœ€å°èˆˆå‘³åº¦é–¾å€¤")
    
    # å®Ÿéš›ã®CLIä½¿ç”¨æ™‚ã¯argparse.parse_args()ã‚’ä½¿ç”¨
    # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    args = argparse.Namespace(
        mode="single",
        duration=300,  # 5åˆ†
        topics=["consciousness", "AI safety", "neural networks"],
        min_interest=0.7
    )
    
    provider = get_provider("ollama", enhanced=True)
    
    # ãƒ¢ãƒƒã‚¯é–¢æ•°ã®å®šç¾©
    async def mock_search(q): 
        return {'results': [{'link': f'https://example.com/{i}', 'title': f'Result {i}'} for i in range(3)]}
    async def mock_fetch(url): 
        return {'content': f'Content from {url}', 'title': f'Title for {url}'}
    
    crawler = AutonomousWebCrawler(provider, mock_search, mock_fetch)
    crawler.min_interest_threshold = args.min_interest
    
    if args.mode == "single":
        print(f"é–‹å§‹: å˜ç™ºå­¦ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ ({args.duration}ç§’)")
        print(f"æ¢ç´¢ãƒˆãƒ”ãƒƒã‚¯: {', '.join(args.topics)}")
        print(f"æœ€å°èˆˆå‘³åº¦: {args.min_interest}")
        
        result = await crawler.start_autonomous_learning(
            initial_topics=args.topics,
            session_duration=args.duration
        )
        
        print(f"\nå­¦ç¿’å®Œäº†:")
        print(f"  æ¢ç´¢ãƒšãƒ¼ã‚¸: {result['pages_crawled']}")
        print(f"  èˆˆå‘³æ·±ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {len(result['session_summary']['content_discovered'])}")
        print(f"  å­¦ç¿’åŠ¹ç‡: {result['learning_efficiency']:.2f}")
        
    elif args.mode == "continuous":
        manager = ContinuousLearningManager(provider, mock_search, mock_fetch)
        
        print("ç¶™ç¶šå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šä¸­...")
        setup = await manager.setup_continuous_learning()
        print(f"è¨­å®šå®Œäº†: {setup['continuous_learning_setup']}")
        
        # ãƒ‡ãƒ¢ç”¨ã«æ—¥æ¬¡ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
        print("æ—¥æ¬¡æ¢ç´¢ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
        session = await manager.execute_scheduled_learning("daily_exploration")
        print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†: {session['session_completed']}")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ CogniQuantumè‡ªå¾‹Webå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢ ğŸŒŸ\n")
    
    try:
        # åŸºæœ¬çš„ãªè‡ªå¾‹å­¦ç¿’ãƒ‡ãƒ¢
        await demo_autonomous_learning()
        print("\n" + "="*60 + "\n")
        
        # èˆˆå‘³ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ãƒ¢
        await demo_interest_profiling()
        print("\n" + "="*60 + "\n")
        
        # é«˜åº¦ãªæ©Ÿèƒ½ãƒ‡ãƒ¢
        await demo_advanced_learning_features()
        print("\n" + "="*60 + "\n")
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ‡ãƒ¢
        await demo_learning_integration()
        print("\n" + "="*60 + "\n")
        
        # CLIãƒ„ãƒ¼ãƒ«ãƒ‡ãƒ¢
        await cli_autonomous_learning()
        
        print("\nğŸ‰ å…¨ã¦ã®ãƒ‡ãƒ¢ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        logger.error(f"ãƒ‡ãƒ¢å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

if __name__ == "__main__":
    asyncio.run(main())